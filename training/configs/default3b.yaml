base_model: Qwen/Qwen2.5-3B-Instruct
# Automatically upload checkpoint and final model to HF
hub_model_id: Rexhaif/Qwen2.5-3B-MT-Eval-Reasoner

load_in_8bit: false
load_in_4bit: false
strict: false

#torch_compile: true

rl: grpo
trl:
  temperature: 0.6
  beta: 0.001
  max_completion_length: 2048
  use_vllm: true
  reward_funcs:
    - metric_utils.reward_answer_correctness
    - metric_utils.reward_format_correctness
  vllm_gpu_memory_utilization: 0.9
  vllm_device: "cuda:0"
  vllm_max_model_len: 3072
  vllm_enable_prefix_caching: false
  num_generations: 8

chat_template: qwen_25
datasets:
  - path: allenai/RLVR-GSM-MATH-IF-Mixed-Constraints
    split: "train"
    type: metric_utils.tulu_transform
  - path: Rexhaif/wmt23-pairs
    split: "train[:30%]"
    type: metric_utils.pairwise_ranking_grpo_transform

shuffle_merged_datasets: true

dataset_prepared_path: /workspace/data/last_run_prepared
skip_prepare_dataset: true
val_set_size: 0.0
output_dir: ./outputs/default-3b

dataloader_prefetch_factor: 32
dataloader_num_workers: 2
dataloader_pin_memory: true

gc_steps: 1

sequence_len: 4096
sample_packing: false
eval_sample_packing: false
pad_to_sequence_len: false

wandb_project: llm-reasoning-mt-eval
wandb_entity:
wandb_name: qw2.5-3b-math-0.3epoch

plugins:
  - axolotl.integrations.liger.LigerPlugin
liger_rope: true
liger_rms_norm: true
liger_glu_activation: true
liger_layer_norm: true
liger_fused_linear_cross_entropy: true
gradient_accumulation_steps: 4
micro_batch_size: 32  # should match num_generations / num_gpus

optimizer: adamw_torch_fused
lr_scheduler: cosine
learning_rate: 5.0e-6
cosine_min_lr_ratio: 1.0e-7
max_grad_norm: 1.0
weight_decay: 0.1

bf16: true
tf32: true

gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
flash_attention: true
flash_attn_fuse_qkv: true
flash_attn_fuse_mlp: true
auto_resume_from_checkpoints: true

n_epochs: 1
logging_steps: 25
warmup_steps: 117
evals_per_epoch: 10
saves_per_epoch: 10
save_total_limit: 3
#max_steps: 5000
seed: 42
val_set_size: 0.01

#adapter: lora
#lora_r: 8
#lora_alpha: 16
#lora_dropout: 0.05
#lora_target_linear: true
#lora_mlp_kernel: true
#lora_qkv_kernel: true
#lora_o_kernel: true
