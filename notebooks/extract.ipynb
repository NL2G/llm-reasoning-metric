{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mt_metrics_eval import data\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import xxhash\n",
    "\n",
    "\n",
    "def example_id(src_lang, tgt_lang, src, ref, hyp, system):\n",
    "    return xxhash.xxh128_hexdigest(f'{src_lang}-{tgt_lang}@{system}##{src}##{ref}##{hyp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_s = data.EvalSet('wmt23', 'cs-uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_data(eval_set: data.EvalSet, score_name: str = 'mqm'):\n",
    "    result = []\n",
    "    src_lang = eval_set.src_lang\n",
    "    tgt_lang = eval_set.tgt_lang\n",
    "    scores = eval_set.Scores(level='seg', scorer=score_name)\n",
    "    hypotheses = eval_set.sys_outputs\n",
    "    systems = set(hypotheses.keys()) - {'synthetic_ref', 'refB'}\n",
    "    for i, (src, ref) in enumerate(zip(eval_set.src, eval_set.all_refs[eval_set.std_ref])):\n",
    "        for system in systems:\n",
    "            hyp = hypotheses[system][i]\n",
    "            score = scores[system][i]\n",
    "            if score is not None:\n",
    "                result.append({\n",
    "                    'lp': f\"{src_lang}-{tgt_lang}\",\n",
    "                    'src': src,\n",
    "                    'ref': ref,\n",
    "                    'hyp': hyp,\n",
    "                    'system': system,\n",
    "                    'score': score,\n",
    "                    'score_name': score_name,\n",
    "                    'example_id': example_id(src_lang, tgt_lang, src, ref, hyp, system)\n",
    "                })\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lps(comp: str, score: str = 'mqm'):\n",
    "    lps = []\n",
    "    for lp, meta in data.meta_info.DATA[comp].items():\n",
    "        if 'seg' in meta.std_gold and meta.std_gold['seg'] == score:\n",
    "            lps.append(lp)\n",
    "\n",
    "    return lps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_sqm = pd.concat([\n",
    "    pd.DataFrame(get_all_data(data.EvalSet('wmt23', lp), score_name='da-sqm')) for lp in get_lps('wmt23', 'da-sqm')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_mqm = pd.concat([\n",
    "    pd.DataFrame(get_all_data(data.EvalSet('wmt24', lp), score_name='mqm')) for lp in get_lps('wmt24', 'mqm')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rexhaif/llm-reasoning-metric/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt23_24 = ds.DatasetDict({\n",
    "    'train': ds.Dataset.from_pandas(all_data_sqm),\n",
    "    'test': ds.Dataset.from_pandas(all_data_mqm)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt23_24 = wmt23_24.remove_columns(['__index_level_0__'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method push_to_hub in module datasets.dataset_dict:\n",
      "\n",
      "push_to_hub(repo_id, config_name: str = 'default', set_default: Optional[bool] = None, data_dir: Optional[str] = None, commit_message: Optional[str] = None, commit_description: Optional[str] = None, private: Optional[bool] = None, token: Optional[str] = None, revision: Optional[str] = None, create_pr: Optional[bool] = False, max_shard_size: Union[str, int, NoneType] = None, num_shards: Optional[dict[str, int]] = None, embed_external_files: bool = True) -> huggingface_hub.hf_api.CommitInfo method of datasets.dataset_dict.DatasetDict instance\n",
      "    Pushes the [`DatasetDict`] to the hub as a Parquet dataset.\n",
      "    The [`DatasetDict`] is pushed using HTTP requests and does not need to have neither git or git-lfs installed.\n",
      "\n",
      "    Each dataset split will be pushed independently. The pushed dataset will keep the original split names.\n",
      "\n",
      "    The resulting Parquet files are self-contained by default: if your dataset contains [`Image`] or [`Audio`]\n",
      "    data, the Parquet files will store the bytes of your images or audio files.\n",
      "    You can disable this by setting `embed_external_files` to False.\n",
      "\n",
      "    Args:\n",
      "        repo_id (`str`):\n",
      "            The ID of the repository to push to in the following format: `<user>/<dataset_name>` or\n",
      "            `<org>/<dataset_name>`. Also accepts `<dataset_name>`, which will default to the namespace\n",
      "            of the logged-in user.\n",
      "        config_name (`str`):\n",
      "            Configuration name of a dataset. Defaults to \"default\".\n",
      "        set_default (`bool`, *optional*):\n",
      "            Whether to set this configuration as the default one. Otherwise, the default configuration is the one\n",
      "            named \"default\".\n",
      "        data_dir (`str`, *optional*):\n",
      "            Directory name that will contain the uploaded data files. Defaults to the `config_name` if different\n",
      "            from \"default\", else \"data\".\n",
      "\n",
      "            <Added version=\"2.17.0\"/>\n",
      "        commit_message (`str`, *optional*):\n",
      "            Message to commit while pushing. Will default to `\"Upload dataset\"`.\n",
      "        commit_description (`str`, *optional*):\n",
      "            Description of the commit that will be created.\n",
      "            Additionally, description of the PR if a PR is created (`create_pr` is True).\n",
      "\n",
      "            <Added version=\"2.16.0\"/>\n",
      "        private (`bool`, *optional*):\n",
      "            Whether to make the repo private. If `None` (default), the repo will be public unless the\n",
      "            organization's default is private. This value is ignored if the repo already exists.\n",
      "        token (`str`, *optional*):\n",
      "            An optional authentication token for the Hugging Face Hub. If no token is passed, will default\n",
      "            to the token saved locally when logging in with `huggingface-cli login`. Will raise an error\n",
      "            if no token is passed and the user is not logged-in.\n",
      "        revision (`str`, *optional*):\n",
      "            Branch to push the uploaded files to. Defaults to the `\"main\"` branch.\n",
      "\n",
      "            <Added version=\"2.15.0\"/>\n",
      "        create_pr (`bool`, *optional*, defaults to `False`):\n",
      "            Whether to create a PR with the uploaded files or directly commit.\n",
      "\n",
      "            <Added version=\"2.15.0\"/>\n",
      "        max_shard_size (`int` or `str`, *optional*, defaults to `\"500MB\"`):\n",
      "            The maximum size of the dataset shards to be uploaded to the hub. If expressed as a string, needs to be digits followed by a unit\n",
      "            (like `\"500MB\"` or `\"1GB\"`).\n",
      "        num_shards (`Dict[str, int]`, *optional*):\n",
      "            Number of shards to write. By default, the number of shards depends on `max_shard_size`.\n",
      "            Use a dictionary to define a different num_shards for each split.\n",
      "\n",
      "            <Added version=\"2.8.0\"/>\n",
      "        embed_external_files (`bool`, defaults to `True`):\n",
      "            Whether to embed file bytes in the shards.\n",
      "            In particular, this will do the following before the push for the fields of type:\n",
      "\n",
      "            - [`Audio`] and [`Image`] removes local path information and embed file content in the Parquet files.\n",
      "\n",
      "    Return:\n",
      "        huggingface_hub.CommitInfo\n",
      "\n",
      "    Example:\n",
      "\n",
      "    ```python\n",
      "    >>> dataset_dict.push_to_hub(\"<organization>/<dataset_id>\")\n",
      "    >>> dataset_dict.push_to_hub(\"<organization>/<dataset_id>\", private=True)\n",
      "    >>> dataset_dict.push_to_hub(\"<organization>/<dataset_id>\", max_shard_size=\"1GB\")\n",
      "    >>> dataset_dict.push_to_hub(\"<organization>/<dataset_id>\", num_shards={\"train\": 1024, \"test\": 8})\n",
      "    ```\n",
      "\n",
      "    If you want to add a new configuration (or subset) to a dataset (e.g. if the dataset has multiple tasks/versions/languages):\n",
      "\n",
      "    ```python\n",
      "    >>> english_dataset.push_to_hub(\"<organization>/<dataset_id>\", \"en\")\n",
      "    >>> french_dataset.push_to_hub(\"<organization>/<dataset_id>\", \"fr\")\n",
      "    >>> # later\n",
      "    >>> english_dataset = load_dataset(\"<organization>/<dataset_id>\", \"en\")\n",
      "    >>> french_dataset = load_dataset(\"<organization>/<dataset_id>\", \"fr\")\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(wmt23_24.push_to_hub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 96/96 [00:00<00:00, 2154.52ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 27/27 [00:00<00:00, 1550.00ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Rexhaif/wmt23-24/commit/f86bd4f46c832238dcf5da256fe3851d60c0799a', commit_message='Upload dataset', commit_description='', oid='f86bd4f46c832238dcf5da256fe3851d60c0799a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/Rexhaif/wmt23-24', endpoint='https://huggingface.co', repo_type='dataset', repo_id='Rexhaif/wmt23-24'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmt23_24.push_to_hub(\n",
    "    \"Rexhaif/wmt23-24\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
